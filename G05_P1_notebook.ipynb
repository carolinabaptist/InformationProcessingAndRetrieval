{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 5**\n",
    "- Ana Carolina Baptista, 95529\n",
    "- Ådne Tøftum Svendsrud, 108703\n",
    "- António Martinho do Rosário Marçal, 95735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from time import time\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, Tree\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import RegexpParser\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, fbeta_score, precision_recall_curve, precision_score, roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def tokenization(file):\n",
    "    s = sent_tokenize(file)\n",
    "    w = []\n",
    "    for i in range(len(s)):\n",
    "        w = w + word_tokenize(s[i])\n",
    "\n",
    "    # each term needs a tag\n",
    "    tagged_tokens = pos_tag(w)\n",
    "\n",
    "    # define a rule for noun phrases: article + adjective + noun*\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    extract = RegexpParser(grammar)\n",
    "    res = extract.parse(tagged_tokens)\n",
    "    \n",
    "    res_list = []\n",
    "    for subtree in res:\n",
    "        if type(subtree) == Tree and subtree.label() == 'NP':\n",
    "            res_list.append(list(subtree))\n",
    "\n",
    "    res_final = res_list + [[token] for token in tagged_tokens]\n",
    "\n",
    "    without_tags = [[w for (w, tag) in lista] for lista in res_final]\n",
    "\n",
    "    return without_tags\n",
    "\n",
    "\n",
    "def preprocess(word_tokens, stopwords=[]):\n",
    "    \n",
    "    # Lowercasing, removing stop words and ponctuation\n",
    "    return_list = []\n",
    "    ps = PorterStemmer()\n",
    "    for term in word_tokens:\n",
    "        filtered_terms = [w for w in term if w.lower() not in stopwords and w.lower() not in punctuation]\n",
    "        \n",
    "        #stemming\n",
    "        stemmed_terms = [ps.stem(w) for w in filtered_terms]\n",
    "\n",
    "        if len(stemmed_terms):\n",
    "            return_list.append(tuple(stemmed_terms))\n",
    "\n",
    "    return return_list\n",
    "\n",
    "def indexing(D, stopwords_language='english'):\n",
    "    start_time = time()\n",
    "    index = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for i, genres in enumerate(tqdm(listdir(D), desc=\"Indexing Genres\")): \n",
    "        for j, doc in enumerate(listdir(f\"{D}/{genres}\")):  \n",
    "            # parsing\n",
    "            if not doc.endswith('.txt'):\n",
    "                continue\n",
    "\n",
    "            with open(f\"{D}/{genres}/{doc}\", 'r') as d:\n",
    "                # Skip the title\n",
    "                next(d)\n",
    "                # tokenization\n",
    "                tokens = tokenization(d.read())\n",
    "\n",
    "                #stop words\n",
    "                stop_words = set(stopwords.words(stopwords_language))\n",
    "\n",
    "                # removing stop words\n",
    "                normalized_tokens = preprocess(tokens, stop_words)\n",
    "\n",
    "                for token in normalized_tokens:\n",
    "                                      \n",
    "                    index[token][i][j] += 1\n",
    "\n",
    "    final_index = {word: [(genre_idx, doc_idx, freq) for genre_idx, genre_docs in docs.items() for doc_idx, freq in\n",
    "                          genre_docs.items()] for word, docs in index.items()}\n",
    "\n",
    "    return final_index, (time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(d, I, p=8, l=500, o=\"relevance\", stopwords_language='english', scoring=\"dfidf\", num_docs=2225, bert_mode='cls', bert_optype='sumsum', rrf=False, mmr=False, mmr_lambda=0.5):\n",
    "    \n",
    "    with open(d, 'r', encoding=\"utf-8\") as f:\n",
    "        doc_text = f.read()\n",
    "\n",
    "    summary = []\n",
    "    if scoring == \"bert\" or rrf or mmr:\n",
    "        sentences = sent_tokenize(doc_text)\n",
    "\n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            embeddings.append(get_bert_output(bert_tokenizer, bert_model, sentence, bert_mode, bert_optype))\n",
    "\n",
    "        if not mmr:\n",
    "            # Calculate cosine similarity between each sentence and the document\n",
    "            for i, sentence in enumerate(embeddings):\n",
    "                score = 0\n",
    "                for j, other_sentence in enumerate(embeddings):\n",
    "                    if i != j:\n",
    "                        score += np.dot(sentence, other_sentence) / (np.linalg.norm(sentence) * np.linalg.norm(other_sentence))\n",
    "                summary.append((i, score))\n",
    "\n",
    "        else:\n",
    "            summary = maximum_marginal_relevance(embeddings)\n",
    "\n",
    "    # Preprocess sentences\n",
    "    stopwords_list = set(stopwords.words(stopwords_language))\n",
    "    preprocessed_sentences = [preprocess(word_tokenize(sentence), stopwords_list) for sentence in sentences]\n",
    "    if scoring != \"dfidf\" and scoring != \"BM25\" and scoring != \"bert\":\n",
    "        raise ValueError(\"Invalid value for 'scoring'. Use 'dfidf', 'bert' or 'BM25'.\")\n",
    "\n",
    "    score_options = []\n",
    "    if not mmr and (rrf or scoring == \"dfidf\"):\n",
    "        score_options.append(find_dfidf(I, num_docs, d))\n",
    "    if not mmr and (rrf or scoring == \"BM25\"):\n",
    "        score_options.append(find_BM25(I, num_docs, d))\n",
    "\n",
    "    summary_options = []\n",
    "    if summary:\n",
    "        summary_options.append(summary)\n",
    "\n",
    "    for scores in score_options:\n",
    "        summary = []\n",
    "        for i, sentence_terms in enumerate(preprocessed_sentences):\n",
    "            score = 0\n",
    "            for term in sentence_terms:\n",
    "                if term in I:\n",
    "                    if term not in scores:\n",
    "                        \n",
    "                        scores[term] = 0\n",
    "                    score += scores[term]  \n",
    "\n",
    "            summary.append((i, score))\n",
    "\n",
    "        if o == \"appearance\":\n",
    "            summary = sorted(summary, key=lambda x: x[0])  # Sorts by sentence position\n",
    "        elif o == \"relevance\":\n",
    "            summary = sorted(summary, key=lambda x: x[1], reverse=True)  # Sorts by score, highest first\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'o'. Use 'appearance' or 'relevance'.\")\n",
    "        \n",
    "        summary_options.append(summary)\n",
    "    \n",
    "    optimal_summary = []\n",
    "    if len(summary_options) > 1:\n",
    "        optimal_summary = reciprocal_rank_fusion(summary_options)\n",
    "    else:\n",
    "        optimal_summary = summary_options[0]\n",
    "\n",
    "    \n",
    "\n",
    "    # Find out if the summary should be based on the maximum number of sentences or characters\n",
    "    selected_sentences = []\n",
    "    current_length = 0\n",
    "    for i, (sentence_index, score) in enumerate(optimal_summary):\n",
    "        sentence = sentences[sentence_index]\n",
    "        current_length += len(sentence)\n",
    "\n",
    "        if current_length <= l:\n",
    "            selected_sentences.append((sentence_index, score))\n",
    "        else:\n",
    "            break\n",
    "        if len(selected_sentences) == p:\n",
    "            break\n",
    "\n",
    "    return selected_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_id(doc):\n",
    "    folder_ids = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "    valores = [0, 1, 2, 3, 4]\n",
    "    dic = {folder_id: valor for folder_id, valor in zip(folder_ids, valores)}\n",
    "    for genre in folder_ids:\n",
    "        if genre in doc:\n",
    "            return dic[genre]\n",
    "\n",
    "\n",
    "def file_id(doc):\n",
    "    path = str(doc)\n",
    "    parts = path.split('\\\\')\n",
    "    return int(parts[-1].split('.')[0]) - 1\n",
    "\n",
    "\n",
    "def find_dfidf(I, N, d):\n",
    "    fi_id = file_id(d)\n",
    "    fo_id = folder_id(d)\n",
    "\n",
    "    # doc frequency\n",
    "    df = []\n",
    "    for term in I:\n",
    "        df = df + [[term, len(I[term])]]\n",
    "\n",
    "    filtered = {}\n",
    "    # select words of selected doc\n",
    "    for term in list(I.keys()):\n",
    "        for tupla in I[term]:\n",
    "            if tupla[0] == fo_id and tupla[1] == fi_id:\n",
    "                filtered[term] = tupla\n",
    "\n",
    "    for ele in df:\n",
    "        if ele[0] not in filtered:\n",
    "            df.remove(ele)\n",
    "\n",
    "    idf = {}\n",
    "    for ele in df:\n",
    "        idf[ele[0]] = math.log10(N / ele[1])\n",
    "\n",
    "    tf = {}\n",
    "\n",
    "    # term frequency of selected docs\n",
    "    for ele in filtered:\n",
    "        tf[ele] = filtered[ele][2]\n",
    "\n",
    "    # 1 + log TF\n",
    "    for ele in tf:\n",
    "        tf[ele] = 1 + math.log10(tf[ele])\n",
    "\n",
    "    tfidf = {key: tf[key] * idf[key] for key in tf}\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "def find_BM25(I, N, d, k=1.2, b=0.75):\n",
    "    fi_id = file_id(d)\n",
    "    fo_id = folder_id(d)\n",
    "\n",
    "    # doc frequency - number of documents a term appears\n",
    "    df = []\n",
    "    for term in I:\n",
    "        df = df + [[term, len(I[term])]]\n",
    "\n",
    "    filtered = {}\n",
    "    # select words of selected doc\n",
    "    for term in list(I.keys()):\n",
    "        for tupla in I[term]:\n",
    "            if tupla[0] == fo_id and tupla[1] == fi_id:\n",
    "                filtered[term] = tupla\n",
    "\n",
    "    for ele in df:\n",
    "        if ele[0] not in filtered:\n",
    "            df.remove(ele)\n",
    "\n",
    "    idf = {}\n",
    "    for ele in df:\n",
    "        idf[ele[0]] = math.log10(\n",
    "            1 + ((N - ele[1] + 0.5) / (ele[1] + 0.5)))  # Same as log(1 + (N - n + 0.5) / (n + 0.5))\n",
    "\n",
    "    tf = {}\n",
    "    # term frequency of selected docs\n",
    "    for ele in filtered:\n",
    "        tf[ele] = filtered[ele][2]\n",
    "\n",
    "    avgdl = 0\n",
    "    for ele in df:\n",
    "        avgdl = avgdl + ele[1]\n",
    "    avgdl = avgdl / len(df)\n",
    "\n",
    "    BM25 = {}\n",
    "    for ele in tf:\n",
    "        BM25[ele] = idf[ele] * (tf[ele] * (k + 1)) / (\n",
    "                tf[ele] + k * (1 - b + b * (len(df) / avgdl))) \n",
    "\n",
    "    return BM25\n",
    "\n",
    "\n",
    "def get_bert_output(tokenizer, model, sentence, mode='cls', optype='sumsum'):\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    tokens_tensor = torch.tensor([tokenizer.convert_tokens_to_ids(tokenized_text)])\n",
    "    segments_tensors = torch.tensor([[1] * len(tokenized_text)])\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    if mode == 'cls':\n",
    "        embedding = outputs[\"last_hidden_state\"].squeeze()[0]\n",
    "    elif mode == 'pooled':\n",
    "        embedding = outputs[\"pooler_output\"].squeeze()\n",
    "    else:  # 'hidden'\n",
    "        layers = torch.stack(outputs['hidden_states'][-4:])\n",
    "        if optype == \"sumsum\":\n",
    "            embedding = torch.sum(layers.sum(0).squeeze(), dim=0)\n",
    "        elif optype == \"summean\":\n",
    "            embedding = torch.sum(layers.mean(0).squeeze(), dim=0)\n",
    "        elif optype == \"meanmean\":\n",
    "            embedding = torch.mean(layers.mean(0).squeeze(), dim=0)\n",
    "        else:\n",
    "            embedding = torch.mean(layers.sum(0).squeeze(), dim=0)\n",
    "    return embedding.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(sum_options, y = 5):\n",
    "        \n",
    "    optimal = [[x, 0] for x in range(len(sum_options[0]))]\n",
    "    for opt in sum_options:\n",
    "\n",
    "        for i, sentence in enumerate(opt):\n",
    "            optimal[sentence[0]][1] += 1 / (y + i)\n",
    "\n",
    "    optimal = [tuple(x) for x in optimal]\n",
    "    return sorted(optimal, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_marginal_relevance(embedings, mmr_lambda=0.5):\n",
    "\n",
    "    document = [(i, embedings[i]) for i in range(len(embedings))]\n",
    "    summary = []\n",
    "\n",
    "    while len(document) > 0:\n",
    "\n",
    "        max_mmr = (0, -2, 0)\n",
    "        for sentence in document:\n",
    "            \n",
    "            sim_s_d = np.mean([cosine_similarity([sentence[1]], [array])[0, 0] for array in embedings])\n",
    "\n",
    "            if summary:\n",
    "                sim_s_v = np.mean([cosine_similarity([sentence[1]], [x[2]])[0, 0] for x in summary])\n",
    "            else:\n",
    "                sim_s_v = 0\n",
    "\n",
    "            mmr_score = (1 - mmr_lambda) * sim_s_d - mmr_lambda * sim_s_v\n",
    "            if mmr_score > max_mmr[1]:\n",
    "                max_mmr = (sentence[0], mmr_score, sentence[1])\n",
    "\n",
    "        document = [tup for tup in document if tup[0] != max_mmr[0]]\n",
    "        summary.append(max_mmr)\n",
    "\n",
    "    return [s[:2] for s in summary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def folder_id(doc):\n",
    "    folder_ids = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "    valores = [0, 1, 2, 3, 4]\n",
    "    dic = {folder_id: valor for folder_id, valor in zip(folder_ids, valores)}\n",
    "    for genre in folder_ids:\n",
    "        if genre in doc:\n",
    "            return dic[genre]\n",
    "\n",
    "\n",
    "def file_id(doc):\n",
    "    path = str(doc)\n",
    "    parts = path.split('\\\\')\n",
    "    return int(parts[-1].split('.')[0]) - 1\n",
    "\n",
    "\n",
    "def find_dfidf(I, N, d):\n",
    "    fi_id = file_id(d)\n",
    "    fo_id = folder_id(d)\n",
    "\n",
    "    # doc frequency\n",
    "    df = []\n",
    "    for term in I:\n",
    "        df = df + [[term, len(I[term])]]\n",
    "\n",
    "    filtered = {}\n",
    "    # select words of selected doc\n",
    "    for term in list(I.keys()):\n",
    "        for tupla in I[term]:\n",
    "            if tupla[0] == fo_id and tupla[1] == fi_id:\n",
    "                filtered[term] = tupla\n",
    "\n",
    "    for ele in df:\n",
    "        if ele[0] not in filtered:\n",
    "            df.remove(ele)\n",
    "\n",
    "    idf = {}\n",
    "    for ele in df:\n",
    "        idf[ele[0]] = math.log10(N / ele[1])\n",
    "\n",
    "    tf = {}\n",
    "    \n",
    "    #term frequency of selected docs\n",
    "    for ele in filtered:\n",
    "        tf[ele] = filtered[ele][2]\n",
    "\n",
    "    # 1 + log TF\n",
    "    for ele in tf:\n",
    "        tf[ele] = 1 + math.log10(tf[ele])\n",
    "\n",
    "    tfidf = {key: tf[key] * idf[key] for key in tf}\n",
    "    return tfidf\n",
    "\n",
    "def keyword_extraction(d, p, I, n=2225):\n",
    "    tfidf = find_dfidf(I, n, d)\n",
    "\n",
    "    # normalization\n",
    "    # vn -> vn^2\n",
    "    tfidf_square = {}\n",
    "    for ele in tfidf:\n",
    "        tfidf_square[ele] = tfidf[ele] ** 2\n",
    "\n",
    "    # sum of vn^2\n",
    "    tfidf_sum = 0\n",
    "    for ele in tfidf_square:\n",
    "        tfidf_sum = tfidf_sum + tfidf_square[ele]\n",
    "    # square of sum\n",
    "    tfidf_square = math.sqrt(tfidf_sum)\n",
    "\n",
    "    # tfidf / square of sum\n",
    "    for ele in tfidf:\n",
    "        tfidf[ele] = tfidf[ele] / tfidf_square\n",
    "\n",
    "    # ordered normalized values\n",
    "    norm_ordered = dict(sorted(tfidf.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    final_list = []\n",
    "    for ele in norm_ordered:\n",
    "        final_list = final_list + [ele]\n",
    "\n",
    "    return final_list[:p]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_references(base_dir):\n",
    "    references = []\n",
    "\n",
    "    for genre_id, genre in enumerate(tqdm(listdir(f'{base_dir}/Summaries'), desc=\"Vectorizing Genres\")):\n",
    "        for doc_id, doc in enumerate(listdir(f\"{base_dir}/Summaries/{genre}\")):\n",
    "\n",
    "            total_sentences = []\n",
    "            ref_sentences = []\n",
    "            with open(f\"{base_dir}/News Articles/{genre}/{doc}\", 'r') as f:\n",
    "                total_sentences = sent_tokenize(f.read())\n",
    "\n",
    "            with open(f\"{base_dir}/Summaries/{genre}/{doc}\", 'r') as f:\n",
    "                ref_sentences = sent_tokenize(f.read())\n",
    "\n",
    "            common = ()\n",
    "            for i, sentence in enumerate(total_sentences):\n",
    "                for ref in ref_sentences:\n",
    "                    if sentence in ref:\n",
    "                        common += (i,)\n",
    "                        break\n",
    "\n",
    "            references.append(((genre_id, doc_id), common))\n",
    "            # print(((genre_id, doc_id), common))\n",
    "\n",
    "    return references\n",
    "\n",
    "def evaluation(S, Rset, beta=1.75):\n",
    "    def plot_pr_curve(predictions, answers, title=\"\"):\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        # Compute precision-recall pairs for each document\n",
    "        for i in range(len(answers)):\n",
    "            precision, recall, t = precision_recall_curve(answers[i], predictions[i])\n",
    "            plt.plot(recall, precision, marker='x', markersize=10)\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        if not title:\n",
    "            plt.title('Precision-Recall Curve')\n",
    "        else:\n",
    "            plt.title(f\"Precision-Recall Curve for Genre ID = {title}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def collapse_scores(summary_clp):\n",
    "        return tuple([x[0] for x in summary_clp])\n",
    "\n",
    "    def binarize_sets(relevant_summaries, relevant_references):\n",
    "\n",
    "        max_sentences = 0\n",
    "        for i in relevant_summaries + relevant_references:\n",
    "            if max(i) > max_sentences:\n",
    "                max_sentences = max(i)\n",
    "\n",
    "        def tuple_to_binary_labels(tuple_list, max_sentences):\n",
    "            binary_labels = np.zeros(max_sentences, dtype=int)\n",
    "            binary_labels[np.array(tuple_list) - 1] = 1\n",
    "            return binary_labels\n",
    "\n",
    "        binary_summaries = np.array([tuple_to_binary_labels(t, max_sentences) for t in relevant_summaries])\n",
    "        binary_references = np.array([tuple_to_binary_labels(t, max_sentences) for t in relevant_references])\n",
    "\n",
    "        return binary_summaries, binary_references\n",
    "\n",
    "    # Filtering to make sure references exist in the summaries\n",
    "    sum_indexes = [summ[0] for summ in S]\n",
    "    Rset = [ref for ref in Rset if ref[0] in sum_indexes]\n",
    "\n",
    "    total_fscore = 0\n",
    "    total_precision = 0\n",
    "    total_auc = 0\n",
    "    for genre_id in range(5):\n",
    "        relevant_summaries = [collapse_scores(s[1]) for s in S if s[0][0] == genre_id]\n",
    "        relevant_references = [s[1] for s in Rset if s[0][0] == genre_id]\n",
    "\n",
    "        binary_summaries, binary_references = binarize_sets(relevant_summaries, relevant_references)\n",
    "\n",
    "        f_score = fbeta_score(binary_references, binary_summaries, beta=beta, average=\"micro\")\n",
    "        precision = precision_score(binary_references, binary_summaries, average=\"micro\")\n",
    "        auc_score = roc_auc_score(binary_references, binary_summaries, average=\"micro\")\n",
    "        total_fscore += f_score\n",
    "        total_precision += precision\n",
    "        total_auc += auc_score\n",
    "\n",
    "        print(f\"Genre ID = {genre_id}\")\n",
    "        print(f\"\\tF-Measure with β = {beta} --> {f_score}\")\n",
    "        print(f\"\\tPrecision --> {precision}\")\n",
    "        print(f\"\\tAUC --> {auc_score}\")\n",
    "\n",
    "        plot_pr_curve(binary_summaries, binary_references, title=str(genre_id))\n",
    "\n",
    "    # Pre-processing the summaries and references\n",
    "    S = [collapse_scores(s[1]) for s in S]\n",
    "    Rset = [s[1] for s in Rset]\n",
    "    binary_S, binary_Rset = binarize_sets(S, Rset)\n",
    "\n",
    "    print(f\"Average F-Score for all the categories --> {round(total_fscore / 5, 4)}\")\n",
    "    print(f\"The Mean Average Precision (MAP) -->  {round(total_precision / 5, 4)}\")\n",
    "    print(f\"The Average Area Under the ROC Curve (AUC) -->  {round(total_auc / 5, 4)}\")\n",
    "    plot_pr_curve(binary_S, binary_Rset, \"All\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
